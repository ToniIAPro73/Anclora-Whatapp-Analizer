# ğŸ“¦ WhatsApp AI Analyzer MVP - Paquete Completo

## âœ… Contenido del Paquete

Has descargado el proyecto completo listo para ejecutar. El archivo incluye:

### ğŸ“ Estructura completa
- âœ… CÃ³digo fuente completo (18 archivos)
- âœ… ConfiguraciÃ³n base (.env.example)
- âœ… Schema de base de datos (PostgreSQL)
- âœ… Scripts de testing y utilidades
- âœ… DocumentaciÃ³n completa

### ğŸ“ Archivos principales
- `README.md` - DocumentaciÃ³n completa
- `QUICKSTART.md` - GuÃ­a de inicio rÃ¡pido
- `CHANGELOG.md` - Historial de versiones
- `package.json` - Dependencias Node.js
- `index.js` - Punto de entrada
- `sql/schema.sql` - Schema PostgreSQL

---

## ğŸš€ Pasos de InstalaciÃ³n

### 1. Descomprimir

```bash
# Descomprime en tu directorio preferido
cd ~/
tar -xzf whatsapp-ai-analyzer-mvp.tar.gz
cd whatsapp-ai-analyzer-mvp
```

### 2. Instalar dependencias

```bash
npm install
```

Esto instalarÃ¡:
- `@whiskeysockets/baileys` - WhatsApp
- `playwright` - Web scraping
- `pg` - PostgreSQL client
- `winston` - Logging
- Y todas las demÃ¡s dependencias

### 3. Instalar navegador Playwright

```bash
npx playwright install chromium
npx playwright install-deps chromium
```

### 4. Configurar PostgreSQL

```bash
# Conecta a tu container PostgreSQL existente
docker exec -it [nombre_tu_container] psql -U postgres

# Dentro de psql:
CREATE DATABASE whatsapp_ai_analyzer;
\c whatsapp_ai_analyzer
\q

# Aplica el schema (desde tu terminal)
docker exec -i [nombre_tu_container] psql -U postgres -d whatsapp_ai_analyzer < sql/schema.sql
```

### 5. Configurar variables de entorno

```bash
cp .env.example .env
nano .env  # O usa tu editor preferido
```

**Configura estos valores:**

```env
# PostgreSQL (tu configuraciÃ³n actual)
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=tu_password_actual
DB_NAME=whatsapp_ai_analyzer

# Ollama (ajusta segÃºn tu modelo)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b  # âš ï¸ Usa el modelo que tengas instalado

# WhatsApp
WHATSAPP_SESSION_PATH=./auth_info

# Opcional
LOG_LEVEL=info
SCRAPING_TIMEOUT=30000
MAX_RETRIES=2
```

### 6. Verificar instalaciÃ³n

```bash
# Test PostgreSQL
npm run test-db

# Test Ollama (IMPORTANTE: asegÃºrate que el modelo en .env existe)
npm run test-ollama
```

**Ambos deben mostrar âœ…**

### 7. Â¡Iniciar!

```bash
npm start
```

**AparecerÃ¡ un cÃ³digo QR. EscanÃ©alo con WhatsApp:**
1. Abre WhatsApp en tu mÃ³vil
2. ConfiguraciÃ³n â†’ Dispositivos vinculados
3. Vincular dispositivo
4. Escanea el QR

---

## ğŸ¯ Antes de empezar: LISTA DE VERIFICACIÃ“N

Antes de ejecutar `npm start`, asegÃºrate de tener:

- [ ] Node.js 18+ instalado (`node --version`)
- [ ] Docker con PostgreSQL corriendo (`docker ps | grep postgres`)
- [ ] Base de datos `whatsapp_ai_analyzer` creada
- [ ] Schema SQL aplicado (tabla `link_analysis` existe)
- [ ] Ollama corriendo (`ollama list`)
- [ ] Modelo Ollama descargado y configurado en `.env`
- [ ] `.env` configurado con tus credenciales
- [ ] Playwright instalado (`npx playwright --version`)

**Verifica todo con:**

```bash
# Este comando debe pasar sin errores
npm run test-db && npm run test-ollama
```

---

## ğŸ“Š Primer Uso

### 1. EnvÃ­ate un mensaje de prueba

Una vez conectado, envÃ­ate un mensaje con una URL:

```
https://github.com/nodejs/node
```

### 2. Observa los logs

VerÃ¡s en la terminal cÃ³mo se procesa:
- ğŸ“¥ Scraping del contenido
- ğŸ¤– AnÃ¡lisis con Ollama
- ğŸ’¾ Almacenamiento en PostgreSQL

### 3. Consulta resultados

```bash
# Abre una nueva terminal
npm run stats
```

O directamente en PostgreSQL:

```bash
docker exec -it [tu_container] psql -U postgres -d whatsapp_ai_analyzer

# Consulta los 10 mÃ¡s relevantes
SELECT title, categoria, relevancia, url 
FROM link_analysis 
ORDER BY relevancia DESC 
LIMIT 10;
```

---

## ğŸ”§ Modelos Ollama Recomendados

### SegÃºn tu VRAM (4GB):

**OpciÃ³n 1: Llama 3.1 8B (Recomendado)**
```bash
ollama pull llama3.1:8b
```
- Mejor balance calidad/velocidad
- ~5-10s de inferencia
- 4.7GB de VRAM

**OpciÃ³n 2: Mistral 7B**
```bash
ollama pull mistral:7b
```
- MÃ¡s rÃ¡pido
- ~3-7s de inferencia
- 4.1GB de VRAM

**OpciÃ³n 3: Qwen 2.5 7B**
```bash
ollama pull qwen2.5:7b
```
- Excelente para espaÃ±ol
- ~5-10s de inferencia
- 4.4GB de VRAM

**âš ï¸ IMPORTANTE:** DespuÃ©s de descargar, actualiza `OLLAMA_MODEL` en `.env`

---

## ğŸ†˜ SoluciÃ³n de Problemas RÃ¡pida

### Error: "Ollama model not found"

```bash
# 1. Lista tus modelos
ollama list

# 2. Verifica que el nombre en .env coincide EXACTAMENTE
# Ejemplo: si ves "llama3.1:8b", usa eso en .env

# 3. Si no tienes modelos, descarga uno:
ollama pull llama3.1:8b
```

### Error: "PostgreSQL connection failed"

```bash
# 1. Verifica que estÃ¡ corriendo
docker ps | grep postgres

# 2. Si no estÃ¡ corriendo:
docker start [nombre_container]

# 3. Prueba la conexiÃ³n
npm run test-db
```

### Error: "Module not found"

```bash
# Reinstala dependencias
rm -rf node_modules package-lock.json
npm install
```

### QR no aparece

```bash
# Limpia sesiÃ³n anterior
rm -rf ./auth_info
npm start
```

---

## ğŸ“š DocumentaciÃ³n Adicional

Una vez instalado, lee estos documentos para profundizar:

1. **README.md** - DocumentaciÃ³n completa
   - Arquitectura detallada
   - ConfiguraciÃ³n avanzada
   - PersonalizaciÃ³n de prompts
   - Consultas SQL Ãºtiles

2. **QUICKSTART.md** - GuÃ­a rÃ¡pida de 5 minutos
   - Comandos condensados
   - Troubleshooting express

3. **CHANGELOG.md** - Historial y roadmap
   - CaracterÃ­sticas actuales
   - PrÃ³ximas mejoras
   - Decisiones de diseÃ±o

---

## ğŸ“ PrÃ³ximos Pasos

Una vez que todo funcione:

1. **Personaliza el prompt** de anÃ¡lisis en `src/ai/ollama-client.js`
2. **Ajusta categorÃ­as** segÃºn tus intereses
3. **Crea consultas SQL** personalizadas para tus necesidades
4. **Automatiza reportes** con cron jobs de `npm run stats`
5. **Explora** agregar mÃ¡s plataformas de scraping

---

## ğŸ’¡ Consejos Finales

- **Logs**: Todo se guarda en `logs/combined.log` y `logs/error.log`
- **SesiÃ³n WhatsApp**: Se guarda en `./auth_info` - no la borres
- **Base de datos**: Haz backups periÃ³dicos con `pg_dump`
- **Modelos**: Experimenta con diferentes modelos para encontrar el mejor balance
- **GPU**: Verifica con `nvidia-smi` que Ollama usa la GPU durante inferencia

---

## ğŸ‰ Â¡Listo para usar!

Si todo estÃ¡ configurado correctamente, ya tienes un sistema completo de anÃ¡lisis automatizado de enlaces usando IA 100% local y privado.

**Â¿Problemas?** Revisa el README.md completo o los logs en `./logs/`

**Â¿Todo funciona?** Â¡Disfruta tu nuevo asistente de anÃ¡lisis! ğŸš€

---

**Desarrollado por Toni Ballesteros**  
antonio@anclora.com
